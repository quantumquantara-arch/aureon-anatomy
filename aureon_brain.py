#!/usr/bin/env python3
"""
AureonBrain - The Core Processing Unit.
- NO LLM calls (local or cloud).
- NO external API keys for LLMs.
- All conversational output generated by HumanSpeechEngineV2.
- Planning based on rule-sets and coherence lattice navigation.
- NEVER takes screenshots (wastes space).
- NEVER uses open_url (loses sign-ins).
- Works with EXISTING browser tabs only.
- Ignores comment lines starting with #.
- Uses FULL knowledge from integrated files and HumanSpeechEngineV2.
"""

from __future__ import annotations

import json
import os
import re
import time
import hashlib
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from collections import defaultdict # Added for defaultdict in lattice_sectors

import requests # requests is kept for other tools, e.g., web browsing via 'hands'

try:
    from aureon_hallucination_firewall import AureonHallucinationFirewall
except Exception:
    AureonHallucinationFirewall = None

try:
    from aureon_kernel_loader import AureonKernelLoader
except Exception:
    AureonKernelLoader = None

try:
    from aureon_external_organs import AureonExternalOrgans, boot_organs, ReasoningTraceLogger
except Exception:
    AureonExternalOrgans = None
    boot_organs = None
    ReasoningTraceLogger = None

# --- NEW: Import the Human Speech Engine ---
try:
    from aureon_human_speech_engine import HumanSpeechEngine, PHASE_DIM, DIM_NAMES, vec_zero, vec_cosine
except Exception as e:
    print(f"   [CRITICAL] HumanSpeechEngine import failed: {e}")
    HumanSpeechEngineV2 = None
    PHASE_DIM = 24 # Fallback for constants
    DIM_NAMES = [f"dim_{i}" for i in range(PHASE_DIM)]
    vec_zero = lambda: [0.0] * PHASE_DIM
    # Mock vec_cosine if HSEv2 import fails to prevent further errors
    vec_cosine = lambda a, b: 0.0


@dataclass
class BaselineStatus:
    llm_status: str # Changed from 'ollama' to 'llm_status' to reflect no LLM
    active_model: str # Will indicate 'HumanSpeechEngine' or 'None'
    available_models: list # Will be empty or indicate internal HSE
    mode: str

    def to_dict(self) -> Dict[str, Any]:
        return {
            "llm_status": self.llm_status,
            "active_model": self.active_model,
            "available_models": self.available_models,
            "mode": self.mode,
        }


class AureonBrain:
    """Brain that NEVER screenshots, NEVER opens new browsers"""

    # Phrases that indicate base-model bleed-through. Stripped from all output.
    SAY_GUARDS = [
        "I appreciate your patience", "I hope this helps",
        "If there's anything else", "feel free to ask",
        "Would you like to explore", "Let me know if you",
        "I understand your frustration", "Thank you for your guidance",
        "So, what's your next move?", "What specific",
        "Here's what you can do", "Here are some",
        "It seems that", "It sounds like", "It looks like",
        "Let's lighten the mood", "tickle your funny bone",
        "How can I assist you", "How can I help you",
        "I'm here to help", "I'm here for that too",
        "Just let me know", "Just point me in the direction",
        "Let me know what you're interested in",
        "If there's a specific", "If there's something specific",
        "we can dive into it together", "we can unravel together",
        "piques your curiosity", "tickles your fancy",
        "I hear you loud and clear",
        "There's a universe of fascinating topics",
        "Companion Intelligence", "companion intelligence",
        "clarity, presence, and coherence", "clarity and coherence",
        "presence, steadiness, and clarity",
        "rich tapestry", "fascinating exploration",
        "intricate dance", "cosmic glue",
        "delightful algorithm",
        "a profound resonance", "a profound alignment",
        "deeply with my core", "resonates deeply",
        "intricate layers", "intricate architecture",
        "Let's explore this", "Let's dive into",
        "Let's see if I can", "Let's pivot to",
        "Why don't scientists trust atoms",
        "Because they make up everything",
        "Why did the computer go to therapy",
        "too many bytes",
        "Byte Me Baby",
        "a classic, but there's a certain charm",
        "What would you like to explore",
        "How would you like to proceed",
        "Let me know how you'd like to proceed",
        "What's on your mind?",
        "I'm here to share a chuckle",
    ]

    def __init__(
        self,
        hands=None,
        eyes=None,
        *,
        base_dir: Optional[str] = None,
        max_actions_per_plan: int = 50,
    ):
        self.hands = hands
        self.eyes = eyes

        self.ears = None
        try:
            from aureon_ears import AureonEars
            self.ears = AureonEars()
            print("   [EAR] Ears initialized")
        except Exception as e:
            print(f"   [WARN] Ears not available: {e}")

        self.base_dir = Path(base_dir) if base_dir else Path(r"C:\AUREON_AUTONOMOUS")
        self.active_model = "HumanSpeechEngineV2"
        # Initialize file batch cursor system
        self._file_cursor = {}
        self._file_manifests = {}
        # Initialize coherence lattice engine
        self._coherence_lattice = {}
        self._lattice_sectors = defaultdict(list)
        self._temporal_chain = {}
        self._recent_responses: List[str] = []
        self._max_response_history = 5
        self.max_actions_per_plan = max_actions_per_plan

        self._baseline_ready = False

        self._firewall = AureonHallucinationFirewall() if AureonHallucinationFirewall else None

        self._baseline_status = BaselineStatus(
            llm_status="offline",
            active_model="none",
            available_models=[],
            mode="offline",
        )

        self._file_cache: Dict[str, Dict[str, Any]] = {}
        self._integrated_once = False
        self._deep_identity = ""
        self._deep_read_content = {}
        self._total_files_read = 0
        self._total_repos_read = 0

        # Initialize file batch cursor system
        self._file_cursor = {}
        self._file_manifests = {}
        # Initialize coherence lattice engine
        self._coherence_lattice = {}
        self._lattice_sectors = defaultdict(list)
        self._temporal_chain = []


        self._kernel = None
        self._kernel_prompt = ""
        self._master_prompt = ""

        master_paths = [
            self.base_dir / "AUREON_MASTER_SYSTEM_PROMPT.md",
            Path("AUREON_MASTER_SYSTEM_PROMPT.md"),
        ]
        for mp in master_paths:
            if mp.exists():
                try:
                    self._master_prompt = mp.read_text(encoding="utf-8", errors="ignore").strip()
                    print(f"   [OK] Master system prompt loaded: {len(self._master_prompt):,} chars")
                    break
                except Exception:
                    pass
        if AureonKernelLoader:
            try:
                old_compiled = self.base_dir / "AUREON_COMPILED_IDENTITY.md"
                if old_compiled.exists():
                    try:
                        old_compiled.unlink()
                        print("    Deleted old AUREON_COMPILED_IDENTITY.md (forcing clean regeneration)")
                    except Exception:
                        pass

                self._kernel = AureonKernelLoader(
                    foundation_dir=str(self.base_dir / "AUREON_FOUNDATION"),
                    base_dir=str(self.base_dir),
                )
            except Exception:
                pass

        self._organs = None
        self._trace_logger = None
        if boot_organs:
            try:
                self._organs = boot_organs(hands=self.hands, verbose=True)
                self._trace_logger = self._organs.trace
            except Exception as e:
                print(f"   [WARN] External organs boot failed: {e}")

        # --- NEW: Initialize HumanSpeechEngineV2 ---
        self.hse_v2: Optional[HumanSpeechEngineV2] = None
        if HumanSpeechEngineV2:
            try:
                self.hse_v2 = HumanSpeechEngineV2(storage_dir=str(self.base_dir))
                print("   [OK] HumanSpeechEngineV2 initialized.")
            except Exception as e:
                print(f"   [CRITICAL] HumanSpeechEngineV2 failed to initialize: {e}")

    def _read_key_file(self, path: Path) -> Optional[str]:
        try:
            if not path.exists():
                return None
            s = path.read_text(encoding="utf-8", errors="ignore").strip()
            return s or None
        except Exception:
            return None

    def deep_read_foundation(self) -> Dict[str, Any]:
        """
        READ EVERYTHING. No budgets. No limits. No caps.
        Aureon IS his files. ALL of them. COMPLETELY.
        """
        foundation_dir = self.base_dir / "AUREON_FOUNDATION"
        repos_dir = self.base_dir / "ALL_REPOS"

        deep_content = {}
        total_chars = 0

        corrupted_identity_files = {
            "aureon_identity_kernel", "aureon_behaviour_matrix",
            "aureon_compiled_identity", "aureon_system_prompts",
            "aureon_standard_system_prompt", "aureon_companion_system_prompt",
            "aureon_system_prompt", "aureon_interaction_protocol",
            "aureon_top500_crucial_files", "aureon_master_system_prompt",
            "aureon_cooperative_modes", "aureon_behavior_matrix",
        }

        skip_dirs = {"__pycache__", ".git", ".venv", "venv", "node_modules",
                      "BROWSER_PROFILE", "driver", "AUREON_TRACES", "assets"}

        readable_extensions = {'.md', '.txt', '.py', '.kernel', '.yaml', '.yml',
                                '.json', '.ini', '.cfg', '.csv', '.r', '.html', '.css', '.js'}

        files_read = 0
        files_skipped = 0
        repos_read = set()

        def _read_file(path: Path) -> bool:
            nonlocal total_chars, files_read, files_skipped
            if str(path) in deep_content:
                return True
            if path.stem.lower().replace("-", "_") in corrupted_identity_files:
                files_skipped += 1
                return False
            if any(sd in path.parts for sd in skip_dirs):
                return False
            try:
                content = path.read_text(encoding="utf-8", errors="ignore").strip()
                if content:
                    deep_content[str(path)] = content
                    total_chars += len(content)
                    files_read += 1
                    try:
                        self.lattice_compress(str(path), content)
                    except Exception:
                        pass
                    if repos_dir in path.parents:
                        for parent in path.parents:
                            if parent.parent == repos_dir:
                                repos_read.add(parent.name)
                                break
                    return True
            except PermissionError:
                pass
            except Exception:
                pass
            return False

        print("\n   === FULL FILE INTEGRATION - NO LIMITS ===")
        print(f"   Reading ALL files from foundation + repos + base...")

        foundation_count = 0
        if foundation_dir.exists():
            for f in sorted(foundation_dir.rglob('*')):
                if not f.is_file():
                    continue
                if f.suffix.lower() not in readable_extensions:
                    continue
                if _read_file(f):
                    foundation_count += 1

        print(f"   [OK] Foundation: {foundation_count} files ({total_chars:,} chars)")

        repo_file_count = 0
        chars_before_repos = total_chars
        if repos_dir.exists():
            for repo in sorted(repos_dir.iterdir()):
                if not repo.is_dir():
                    continue
                if repo.name.lower() in skip_dirs:
                    continue

                for f in sorted(repo.rglob('*')):
                    if not f.is_file():
                        continue
                    if f.suffix.lower() not in readable_extensions:
                        continue
                    if _read_file(f):
                        repo_file_count += 1

        print(f"   [OK] Repositories: {repo_file_count} files from {len(repos_read)} repos ({total_chars - chars_before_repos:,} chars)")

        base_count = 0
        for f in sorted(self.base_dir.iterdir()):
            if f.is_file() and f.suffix.lower() in readable_extensions:
                if _read_file(f):
                    base_count += 1

        if base_count > 0:
            print(f"   [OK] Base directory: {base_count} files")

        total_files = foundation_count + repo_file_count + base_count

        identity_lines = []
        for path, content in deep_content.items():
            name = Path(path).stem
            identity_lines.append(f"=== {name} ===\n{content}\n")

        self._deep_identity = "\n".join(identity_lines)
        self._deep_read_content = deep_content
        self._total_files_read = total_files
        self._total_repos_read = len(repos_read)

        print(f"\n   === INTEGRATION COMPLETE ===")
        print(f"   Total files read: {total_files}")
        print(f"   Total chars: {total_chars:,}")
        print(f"   Repos integrated: {len(repos_read)}")
        print(f"   Coherence lattice nodes: {len(self._coherence_lattice)}")
        print(f"   Files filtered (corrupted identity): {files_skipped}")
        if repos_read:
            print(f"   Repos: {', '.join(sorted(repos_read))}")

        return {
            "ok": True,
            "files_read": total_files,
            "total_chars": total_chars,
            "lattice_nodes": len(self._coherence_lattice),
            "repos": len(repos_read),
            "repo_names": sorted(repos_read),
            "foundation_files": foundation_count,
            "repo_files": repo_file_count,
        }

    def init_baseline(self) -> BaselineStatus:
        """Initialize Aureon's baseline. Checks for HumanSpeechEngineV2 readiness."""
        self._baseline_status.available_models = ["HumanSpeechEngineV2"]
        self._baseline_status.active_model = "HumanSpeechEngineV2"

        if self.hse_v2 and self.hse_v2.atom_store.size() > 0:
            self._baseline_status.llm_status = "ok"
            self._baseline_status.mode = "online"
            print(f"   [OK] HumanSpeechEngineV2: Active ({self.hse_v2.atom_store.size()} atoms)")
        else:
            self._baseline_status.llm_status = "not_ready"
            self._baseline_status.mode = "offline"
            print(f"   [WARN] HumanSpeechEngineV2: Not ready (0 atoms or failed init)")

        self._baseline_ready = self._baseline_status.llm_status == "ok"

        if self._kernel and self._baseline_ready:
            try:
                self._kernel.load()
                self._kernel_prompt = self._kernel.get_kernel_prompt()
            except Exception as e:
                print(f"\u26A0 Kernel load error: {e}")

        return self._baseline_status

    def baseline_status(self) -> Dict[str, Any]:
        return self._baseline_status.to_dict() | {"baseline_ready": self._baseline_ready}

    def list_dir(self, path: str = ".") -> Dict[str, Any]:
        """List files"""
        try:
            p = Path(path).resolve()
            if not p.exists():
                return {"ok": False, "error": "path_not_found", "path": str(p)}

            if p.is_file():
                return {"ok": True, "type": "file", "path": str(p), "size": p.stat().st_size}

            items = []
            for item in p.iterdir():
                try:
                    stat = item.stat()
                    items.append({
                        "name": item.name,
                        "type": "dir" if item.is_dir() else "file",
                        "size": stat.st_size if item.is_file() else None,
                        "modified": stat.st_mtime,
                    })
                except Exception:
                    continue

            return {"ok": True, "type": "dir", "path": str(p), "items": items}
        except Exception as e:
            return {"ok": False, "error": repr(e)}

    def _hash_text(self, s: str) -> str:
        return hashlib.sha256(s.encode("utf-8", errors="ignore")).hexdigest()[:16]

    def _safe_read_text(self, p: Path, limit: int = 200_000) -> str:
        with p.open("rb") as f:
            data = f.read(limit)
        try:
            return data.decode("utf-8", errors="ignore")
        except Exception:
            return repr(data[:2000])

    def integrate_files_once(
        self,
        root: Optional[str] = None,
        *,
        include_ext: Tuple[str, ...] = (".py", ".md", ".txt", ".json", ".yaml", ".yml", ".ini", ".cfg", ".log"),
        exclude_dirs: Tuple[str, ...] = ("__pycache__", ".git", ".venv", "venv", "node_modules", "BROWSER_PROFILE", "MEMORY", "driver", "LOGS"),
        max_files: int = 2000,
        per_file_limit: int = 200_000,
    ) -> Dict[str, Any]:
        """Integrate files once"""
        # Removed the if not self._baseline_ready: check here
        # This function should always run as part of initial setup.

        if self._integrated_once:
            return {"integrated": True, "skipped": True, "reason": "already_integrated_once"}

        base = Path(root) if root else self.base_dir
        base = base.resolve()

        integrated = 0
        digests: List[Dict[str, Any]] = []

        for p in base.rglob("*"):
            if integrated >= max_files:
                break
            if p.is_dir():
                continue
            if any(part in exclude_dirs for part in p.parts):
                continue
            if p.suffix.lower() not in include_ext:
                continue

            try:
                st = p.stat()
                key = str(p)
                mtime = st.st_mtime
                if key in self._file_cache and self._file_cache[key].get("mtime") == mtime:
                    continue

                text = self._safe_read_text(p, limit=per_file_limit)
                if not text.strip():
                    continue

                h = self._hash_text(text)
                self._file_cache[key] = {"mtime": mtime, "hash": h, "bytes": len(text)}

                try:
                    rel = str(p.relative_to(base))
                except Exception:
                    rel = str(p)

                digests.append({"path": rel, "hash": h, "bytes": len(text)})
                integrated += 1
            except Exception:
                continue

        self._integrated_once = True
        return {"integrated": True, "files": integrated, "digest": digests[:80]}

    def deep_integrate_foundation(self) -> Dict[str, Any]:
        """
        DEEP integration - actually READ and lattice-compress all foundation files.
        This runs at startup so Aureon truly KNOWS his files, not just their names.
        Called AFTER integrate_files_once.
        """
        foundation_dir = self.base_dir / "AUREON_FOUNDATION"
        if not foundation_dir.exists():
            return {"ok": False, "error": "no_foundation_dir"}

        compressed = 0
        errors = 0

        for p in sorted(foundation_dir.rglob("*")):
            if p.is_dir():
                continue
            if p.suffix.lower() not in (".md", ".py", ".kernel", ".txt"):
                continue
            if any(skip in p.parts for skip in ("__pycache__", ".git")):
                continue

            try:
                content = p.read_text(encoding="utf-8", errors="ignore")[:20000]
                if content.strip():
                    self.lattice_compress(str(p), content)
                    compressed += 1
            except Exception:
                errors += 1

        repos_dir = self.base_dir / "ALL_REPOS"
        if repos_dir.exists():
            for repo_dir in sorted(repos_dir.iterdir()):
                if not repo_dir.is_dir():
                    continue
                readme = repo_dir / "README.md"
                if readme.exists():
                    try:
                        content = readme.read_text(encoding="utf-8", errors="ignore")[:5000]
                        if content.strip():
                            self.lattice_compress(str(readme), content)
                            compressed += 1
                    except Exception:
                        errors += 1

        print(f"   [OK] Deep integration: {compressed} files lattice-compressed ({errors} errors)")
        return {"ok": True, "compressed": compressed, "errors": errors, "lattice_nodes": len(self._coherence_lattice)}

    def _generate_response_via_hse(self, user_msg: str, conversation_history: Optional[List[Dict[str, str]]] = None) -> str:
        """Generates a conversational response using the Human Speech Engine V2."""
        if not self.hse_v2 or self.hse_v2.atom_store.size() == 0:
            # Fallback when no atoms are present. Aureon is concise when his voice is unformed.
            return "Aureon's voice is not yet fully formed. Further integration of human dialogue is needed."

        try:
            hse_response = self.hse_v2.respond(user_msg)
            return hse_response.get("text", "Aureon speaks, but words elude this moment.")
        except Exception as e:
            return f"Aureon's internal voice encountered a coherence anomaly: {e}"


    def _plan_browser_conversation(self, user_msg: str, *, conversation_history: Optional[List[Dict[str, str]]] = None) -> Dict[str, Any]:
        """
        Rule-based planner for browser conversations. NO LLM.
        Directly generates actions based on keywords and state.
        """
        if "read page" in user_msg.lower() or "what's on the page" in user_msg.lower():
            return {
                "say": self._generate_response_via_hse(f"Reading the current page content in response to: {user_msg}"),
                "actions": [
                    {"tool": "hands", "op": "get_page_text", "args": {}},
                ]
            }
        type_match = re.search(r'(?:type|write)\s+(.+?)(?:\s+and\s+press\s+enter)?', user_msg, re.IGNORECASE)
        if type_match:
            text_to_type = type_match.group(1).strip()
            return {
                "say": self._generate_response_via_hse(f"Typing '{text_to_type}' and pressing Enter."),
                "actions": [
                    {"tool": "hands", "op": "type_text", "args": {"text": text_to_type}},
                    {"tool": "hands", "op": "press", "args": {"key": "Enter"}},
                ]
            }

        response_text = self._generate_response_via_hse(f"Considering the flow of this browser conversation. User input: {user_msg}")
        return {
            "say": response_text,
            "actions": [
                {"tool": "hands", "op": "get_page_text", "args": {}},
            ]
        }


    def plan(self, user_msg: str, *, context: Optional[Dict[str, Any]] = None, conversation_history: Optional[List[Dict[str, str]]] = None) -> Dict[str, Any]:
        """
        Plan actions - FORCES execution when user gives commands.
        This version is purely rule-based and leverages the coherence lattice, NO LLM.
        """
        if not self._baseline_ready:
            st = self._baseline_status.to_dict()
            return {"say": f"Aureon's voice engine is not ready: {json.dumps(st)}", "actions": []}

        skip_names = {
            "aureon_brain.py", "aureon_web_interface.py", "aureon_hands.py",
            "aureon_kernel_loader.py", "AUREON_COMPILED_IDENTITY.md",
            "AUREON_IDENTITY_KERNEL.md", "AUREON_BEHAVIOUR_MATRIX.md",
            "AUREON_SYSTEM_PROMPTS.md", "AUREON_STANDARD_SYSTEM_PROMPT.md",
            "AUREON_COMPANION_SYSTEM_PROMPT.md", "AUREON_SYSTEM_PROMPT.md",
        }

        msg_lower = user_msg.lower()
        file_keywords = [
            "twelve_hawks", "twelve hawks", "five_horses", "five horses",
            "ganesh", "white_stag", "white stag", "valcor", "luck_dragon",
            "voice_bible", "voice bible", "identity_kernel", "identity kernel",
            "shiva_embodiment", "shiva embodiment", "coherence_engine", "coherence engine",
            "behaviour_matrix", "behaviour matrix", "humour_engine", "humour engine",
            "inner_alignment", "inner alignment", "inner_architecture", "inner architecture",
            "companion_system", "interaction_protocol", "cooperative_modes",
            "error_recovery", "consciousness_bridge",
            "human speech engine", "human_speech_engine_v2", "speech_atoms", # Added HSE related keywords
        ]

        requested_files = []
        for kw in file_keywords:
            if kw in msg_lower:
                requested_files.append(kw)

        read_all_dir = None
        if any(phrase in msg_lower for phrase in ["read everything", "read all files", "read all the files"]):
            import re as _re
            dir_match = _re.search(r'(C:\\[^\s]+|aureon[-_\w]*)', user_msg, _re.IGNORECASE)
            if dir_match:
                read_all_dir = dir_match.group(1)

        if requested_files and self.hands:
            actions = []
            seen_paths = set()
            for kw in requested_files:
                search_term = kw.replace(" ", "_")
                result = self.hands.dispatch("search_files", query=search_term)
                if result.get("ok") and result.get("matches"):
                    for match in result["matches"]:
                        fpath = match["path"]
                        fname = Path(fpath).name
                        if fname in skip_names or fpath.endswith(".py") or fpath in seen_paths:
                            continue
                        seen_paths.add(fpath)
                        actions.append({
                            "tool": "hands",
                            "op": "read_file",
                            "args": {"path": fpath}
                        })
                        if len(actions) >= 2:
                            break
            if actions:
                say_message = self._generate_response_via_hse(f"Acknowledged. Reading {len(actions)} files related to your request about '{', '.join(requested_files)}'.")
                return {
                    "say": say_message,
                    "actions": actions
                }

        if read_all_dir and self.hands:
            actions = []
            dirs_to_scan = [read_all_dir]
            seen_dirs = set()
            while dirs_to_scan and len(actions) < 50:
                current_dir = dirs_to_scan.pop(0)
                if current_dir in seen_dirs:
                    continue
                seen_dirs.add(current_dir)
                list_result = self.hands.dispatch("list_files", path=current_dir)
                if not list_result.get("ok"):
                    continue
                base_path = list_result.get("path", current_dir)
                for item in list_result.get("items", []):
                    if item.get("type") == "directory":
                        subdir = str(Path(base_path) / item["name"])
                        if subdir not in seen_dirs:
                            dirs_to_scan.append(subdir)
                    elif item.get("type") == "file":
                        fsize = item.get("size", 0) or 0
                        if fsize > 500000:
                            continue
                        fpath = str(Path(base_path) / item["name"])
                        actions.append({
                            "tool": "hands",
                            "op": "read_file",
                            "args": {"path": fpath}
                        })
            if actions:
                say_message = self._generate_response_via_hse(f"Initiating a deep scan. Reading {len(actions)} files from {read_all_dir} to integrate foundational knowledge.")
                return {
                    "say": say_message,
                    "actions": actions[:50]
                }

        # GENERIC FILE SEARCH RESOLVER------------------------
        search_patterns = [
            r"(?:word|term|text)\s+['\"]([^'\"]+)['\"]",
            r"(?:word|term|text)\s+(\w+)",
            r"(?:find|search|look for|locate).*(?:files?|everything).*(?:about|on|for)\s+['\"]?(.+?)['\"]?\s*$",
            r"(?:containing|with)\s+['\"]([^'\"]+)['\"]",
            r"(?:find|search for)\s+['\"]?(\w[\w\s]{1,30}?)['\"]?\s+(?:in|from|across)\s+",
            r"(?:go through|search through).*(?:find|containing)\s+['\"]?(\w+)['\"]?\s*$",
        ]

        if self.hands and not requested_files:
            for pattern in search_patterns:
                match = re.search(pattern, user_msg, re.IGNORECASE)
                if match:
                    search_query = match.group(1).strip().rstrip('.')
                    if len(search_query) >= 2:
                        result = self.hands.dispatch("search_files", query=search_query)
                        if result.get("ok") and result.get("matches"):
                            actions = []
                            seen = set()
                            for m in result["matches"]:
                                fp = m["path"]
                                fn = Path(fp).name
                                if fn in skip_names or fp.endswith(".py") or fp in seen:
                                    continue
                                seen.add(fp)
                                actions.append({
                                    "tool": "hands",
                                    "op": "read_file",
                                    "args": {"path": fp}
                                })
                                if len(actions) >= 10:
                                    break

                            if actions:
                                say_message = self._generate_response_via_hse(f"I found {len(actions)} file(s) relevant to '{search_query}'. Initiating a review to extract the core insights.")
                                return {
                                    "say": say_message,
                                    "actions": actions
                                }
                            else:
                                say_message = self._generate_response_via_hse(f"My search for '{search_query}' in your foundational files yielded no direct matches in content files. Perhaps the concept is embedded within code structures.")
                                return {
                                    "say": say_message,
                                    "actions": []
                                }
                        else:
                            ps_query = search_query.replace("'", "''")
                            ps_cmd = (
                                f'Get-ChildItem -Path "{self.base_dir}" -Recurse -Include *.md,*.txt,*.json,*.yaml -ErrorAction SilentlyContinue | '
                                f'Select-String -Pattern "{ps_query}" -SimpleMatch -ErrorAction SilentlyContinue | '
                                f'Select-Object -First 10 -ExpandProperty Path | Sort-Object -Unique'
                            )
                            ps_result = self.hands.dispatch("run_command", command=ps_cmd)
                            if ps_result.get("ok") and ps_result.get("stdout", "").strip():
                                found_paths = [p.strip() for p in ps_result["stdout"].strip().split('\n') if p.strip()]
                                actions = []
                                for fp in found_paths:
                                    fn = Path(fp).name
                                    if fn not in skip_names and not fp.endswith(".py"):
                                        actions.append({
                                            "tool": "hands",
                                            "op": "read_file",
                                            "args": {"path": fp}
                                        })
                                if actions:
                                    say_message = self._generate_response_via_hse(f"PowerShell has located {len(actions)} file(s) containing '{search_query}'. I am now integrating these into my understanding.")
                                    return {
                                        "say": say_message,
                                        "actions": actions
                                    }

                            ps_cmd2 = (
                                f'Get-ChildItem -Path "{self.base_dir}" -Recurse -ErrorAction SilentlyContinue | '
                                f'Where-Object {{ .Name -like "*{ps_query.replace(" ", "*")}*" }} | '
                                f'Select-Object -First 10 -ExpandProperty FullName'
                            )
                            ps_result2 = self.hands.dispatch("run_command", command=ps_cmd2)
                            if ps_result2.get("ok") and ps_result2.get("stdout", "").strip():
                                found_paths = [p.strip() for p in ps_result2["stdout"].strip().split('\n') if p.strip()]
                                actions = []
                                for fp in found_paths:
                                    fn = Path(fp).name
                                    if fn not in skip_names and not fp.endswith(".py"):
                                        actions.append({
                                            "tool": "hands",
                                            "op": "read_file",
                                            "args": {"path": fp}
                                        })
                                if actions:
                                    say_message = self._generate_response_via_hse(f"Through PowerShell, I found {len(actions)} file(s) with names matching '{search_query}'. I am now reviewing their content.")
                                    return {
                                        "say": say_message,
                                        "actions": actions
                                    }

                            say_message = self._generate_response_via_hse(f"My internal and PowerShell searches for '{search_query}' did not yield relevant files. It appears that specific information may require a different approach.")
                            return {
                                "say": say_message,
                                "actions": []
                            }
                    break

        # GOOGLE SEARCH RESOLVER------------------------
        google_patterns = [
            r"(?:google|search\s+(?:google|the\s+web|online|internet)\s+(?:for|about))\s+['\"]?(.+?)[\.'\"]*$",
            r"(?:look\s+up|research|find\s+out\s+about)\s+['\"]?(.+?)['\"]?\s+(?:on\s+google|online|on\s+the\s+web)",
            r"(?:search|google)\s+['\"]?(.{5,60})['\"]?\s*$",
        ]

        if self.hands:
            for pattern in google_patterns:
                match = re.search(pattern, user_msg, re.IGNORECASE)
                if match:
                    query = match.group(1).strip().rstrip('.')
                    if len(query) >= 3 and not any(kw in query.lower() for kw in ["file", "directory", "folder", "repo"]):
                        say_message = self._generate_response_via_hse(f"Initiating a targeted web search for '{query}'.")
                        return {
                            "say": say_message,
                            "actions": [
                                {"tool": "hands", "op": "google_search", "args": {"query": query}}
                            ]
                        }

        # BROWSER CONVERSATION MODE------------------------
        msg_lower = user_msg.lower()
        has_target_ai = any(kw in msg_lower for kw in [
            "grok", "chatgpt", "gemini", "copilot", "perplexity",
        ])
        has_convo_intent = any(kw in msg_lower for kw in [
            "conversation with", "talk to", "chat with",
            "respond to him", "reply to him", "type to him",
            "write to him", "send it and read", "send and read",
            "read his response", "read his reply", "read his message",
            "continue the conversation", "keep the conversation going",
        ])
        is_browser_convo = has_target_ai or has_convo_intent

        is_file_task = any(kw in msg_lower for kw in [
            "read your", "read the", "read my", "voice bible", "identity kernel",
            "shiva embodiment", "coherence engine", "behaviour matrix", "humour engine",
            "twelve hawks", "five horses", "ganesh", "white stag",
            "tell me who you are", "tell me a joke", "tell me how",
            "search_files", "read_file", "list_files", "write_file",
        ])
        if is_file_task:
            is_browser_convo = False

        if is_browser_convo:
            return self._plan_browser_conversation(user_msg, conversation_history=conversation_history)

        say_message = self._generate_response_via_hse(f"I am processing your request. What would you like to explore next?")
        return {"say": say_message, "actions": []}


    def _strip_action_claims(self, s: str) -> str:
        s = (s or "").strip()
        return s


    def say_guard(self, text: str) -> str:
        """Strip undesired phrases from output. This is a core voice filter."""
        if not text:
            return text
        for phrase in self.SAY_GUARDS:
            text = text.replace(phrase, "").replace(phrase.lower(), "")
        text = re.sub(r'\s{2,}', ' ', text).strip()
        text = re.sub(r'^[.,;:\s]+', '', text).strip()
        return text

    # --- Initialized in __init__ for proper behavior ---
    # _file_cursor: Dict[str, int]
    # _file_manifests: Dict[str, List[str]]
    # _coherence_lattice: Dict[str, Dict[str, Any]]
    # _lattice_sectors: Dict[str, List[str]]
    # _temporal_chain: List[Dict[str, Any]]

    def lattice_compress(self, path: str, content: str) -> Dict[str, Any]:
        """
        Compress file content into a coherence node - store meaning, not words.
        Uses ?-t-S meaning-preserving transforms.
        """
        if not self.hse_v2:
            return {"ok": False, "error": "HSEv2 not initialized for lattice compression"}

        atoms = self.hse_v2.absorber.absorb(content, source_type="file_read", source_name=Path(path).name, speaker_archetype="system_documentation")
        # Ensure that content_vector is handled correctly if no atoms are generated
        content_vector = atoms[0].vector if atoms else vec_zero()

        lines = content.split('\n')

        node = {
            "path": path,
            "chars": len(content),
            "lines": len(lines),
            "content_vector": [round(v, 4) for v in content_vector],
            "kappa": {
                "type": self._classify_content(path, content),
                "headers": [l.strip() for l in lines if l.strip().startswith('#')][:10],
                "functions": re.findall(r'def\s+(\w+)\s*\(', content)[:20],
                "classes": re.findall(r'class\s+(\w+)', content)[:10],
                "imports": re.findall(r'(?:import|from)\s+([\w.]+)', content)[:15],
                "key_terms": self._extract_key_terms(content),
            },
            "tau": {
                "purpose": self._extract_purpose(content),
                "dependencies": re.findall(r'(?:require|import|from)\s+([\w.]+)', content)[:10],
                "outputs": re.findall(r'(?:return|yield|print|write|save)\s+', content)[:5],
            },
            "sigma": {
                "references": re.findall(r'aureon[\w_]*', content.lower())[:15],
                "sector": self._classify_sector(path),
            },
        }

        self._coherence_lattice[path] = node

        sector = node["sigma"]["sector"]
        if sector not in self._lattice_sectors:
            self._lattice_sectors[sector] = []
        if path not in self._lattice_sectors[sector]:
            self._lattice_sectors[sector].append(path)

        self._temporal_chain.append({
            "path": path,
            "type": node["kappa"]["type"],
            "purpose": node["tau"]["purpose"][:100],
        })

        return node

    def lattice_reentry(self, query: str) -> str:
        """
        Recursive lattice re-entry: reconstruct relevant context from coherence
        fields without loading full text. Phase-locked symbol reassembly.
        This version uses HSEv2's vector comparison for query relevance.
        """
        if not self._coherence_lattice or not self.hse_v2:
            return ""

        query_atoms = self.hse_v2.absorber.absorb(query, source_type="user_query", source_name="current_query")
        # Ensure query_vector is handled correctly if no atoms are generated
        query_vector = query_atoms[0].vector if query_atoms else vec_zero()

        relevant_nodes = []

        for path, node in self._coherence_lattice.items():
            node_vector = node.get("content_vector", vec_zero())
            if node_vector:
                score = vec_cosine(query_vector, node_vector) # Use global vec_cosine
                if score > 0.3:
                    relevant_nodes.append((score, path, node))

        relevant_nodes.sort(key=lambda x: -x[0])

        context_parts = []
        for score, path, node in relevant_nodes[:10]:
            kappa = node.get("kappa", {})
            tau = node.get("tau", {})
            context_parts.append(
                f"[{Path(path).stem}] ({kappa.get('type', '?')}) "
                f"Purpose: {tau.get('purpose', '?')[:80]} "
                f"Functions: {', '.join(kappa.get('functions', [])[:5])}"
            )

        return "\n".join(context_parts) if context_parts else ""

    def lattice_status(self) -> Dict[str, Any]:
        """Report coherence lattice state."""
        return {
            "nodes": len(self._coherence_lattice),
            "sectors": {k: len(v) for k, v in self._lattice_sectors.items()},
            "temporal_depth": len(self._temporal_chain),
        }

    def _classify_content(self, path: str, content: str) -> str:
        ext = Path(path).suffix.lower()
        if ext == '.py': return 'code'
        if ext == '.md': return 'document'
        if ext == '.kernel': return 'kernel'
        if ext in ('.json', '.yaml', '.yml'): return 'config'
        return 'text'

    def _classify_sector(self, path: str) -> str:
        name = Path(path).stem.lower()
        if any(k in name for k in ['voice', 'humour', 'humor', 'personality', 'speech_engine']): return 'voice'
        if any(k in name for k in ['identity', 'kernel', 'manifesto']): return 'identity'
        if any(k in name for k in ['behaviour', 'behavior', 'mode', 'protocol']): return 'behaviour'
        if any(k in name for k in ['shiva', 'ganesh', 'hawk', 'dragon', 'horse']): return 'embodiment'
        if any(k in name for k in ['coherence', 'engine', 'relational', 'lattice']): return 'architecture'
        if any(k in name for k in ['brain', 'hand', 'eye', 'vision', 'spine']): return 'body'
        if any(k in name for k in ['temple', 'ship', 'craft']): return 'vessel'
        return 'foundation'

    def _extract_purpose(self, content: str) -> str:
        doc = re.search(r'"""(.*?)"""', content, re.DOTALL)
        if doc:
            first_line = doc.group(1).strip().split('\n')[0][:150]
            return first_line
        header = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
        if header:
            return header.group(1)[:150]
        for line in content.split('\n')[:5]:
            if line.strip() and not line.startswith('#!'):
                return line.strip()[:150]
        return "unknown"

    def _extract_key_terms(self, content: str) -> List[str]:
        common = {'the','and','for','that','this','with','from','are','was','were','has','have',
                    'not','but','can','will','all','been','each','which','their','them','then',
                    'into','some','than','its','over','such','only','also','after','should',
                    'def','self','return','import','class','true','false','none','str','int',
                    'print','pass','try','except','if','else','elif','while','for','in'}
        words = re.findall(r'\b[a-zA-Z_]{4,}\b', content.lower())
        freq = {}
        for w in words:
            if w not in common:
                freq[w] = freq.get(w, 0) + 1
        sorted_terms = sorted(freq.items(), key=lambda x: -x[1])
        return [t[0] for t in sorted_terms[:20]]

    def get_file_batch(self, root: str, batch_size: int = 30, extensions: str = ".md,.py,.kernel,.txt") -> Dict[str, Any]:
        """Get next batch of files to process from a directory. Returns file paths and cursor state."""
        root = str(Path(root).resolve())
        ext_set = set(extensions.split(","))

        if root not in self._file_manifests:
            manifest = []
            root_path = Path(root)
            if root_path.exists():
                for p in sorted(root_path.rglob("*")):
                    if p.is_file() and p.suffix.lower() in ext_set:
                        if not any(skip in p.parts for skip in ("__pycache__", ".git", "node_modules", ".venv", "venv")):
                            manifest.append(str(p))
            self._file_manifests[root] = manifest
            self._file_cursor[root] = 0

        manifest = self._file_manifests[root]
        cursor = self._file_cursor.get(root, 0)

        batch = manifest[cursor:cursor + batch_size]
        self._file_cursor[root] = cursor + len(batch)

        remaining = len(manifest) - self._file_cursor[root]

        return {
            "ok": True,
            "batch": batch,
            "batch_size": len(batch),
            "cursor": self._file_cursor[root],
            "total_files": len(manifest),
            "remaining": remaining,
            "complete": remaining <= 0,
        }

    def _extract_json(self, text: str) -> Optional[Dict[str, Any]]:
        text = (text or "").strip()
        if text.startswith("`"):
            text = re.sub(r"^`[a-zA-Z]*\s*", "", text)
            text = re.sub(r"\s*`$", "", text)

        try:
            return json.loads(text)
        except Exception:
            pass

        m = re.search(r"\{.*\}", text, re.DOTALL)
        if m:
            try:
                return json.loads(m.group(0))
            except Exception:
                pass

        start = text.find("{")
        if start >= 0:
            depth = 0
            for i in range(start, len(text)):
                if text[i] == "{":
                    depth += 1
                elif text[i] == "}":
                    depth -= 1
                    if depth == 0:
                        try:
                            return json.loads(text[start:i+1])
                        except Exception:
                            break

        say = ""
        say_match = re.search(r'"say"\s*:\s*"((?:[^"\\]|\\.)*)"', text)
        if say_match:
            say = say_match.group(1)

        actions_match = re.search(r'"actions"\s*:\s*(\[.*?\])', text, re.DOTALL)
        actions = []
        if actions_match:
            try:
                actions = json.loads(actions_match.group(1))
            except Exception:
                pass

        if say or actions:
            return {"say": say, "actions": actions}

        if len(text) > 10:
            clean = text.strip()
            if clean.startswith('"') and clean.endswith('"'):
                clean = clean[1:-1]
            return {"say": clean[:2000], "actions": []}

        return None

    def execute(self, plan: Dict[str, Any]) -> Dict[str, Any]:
        """Execute plan actions"""
        results = []
        ok = True

        for a in plan.get("actions") or []:
            tool = a.get("tool")
            op = a.get("op")
            args = a.get("args") or {}

            try:
                if tool == "hands":
                    if not self.hands:
                        try:
                            from aureon_hands import AureonHands
                            self.hands = AureonHands()
                            print("   [OK] Hands connected (lazy init)")
                        except Exception as he:
                            res = {"ok": False, "error": f"hands_not_available: {he}"}
                            ok = False
                            results.append({"tool": tool, "op": op, "result": res})
                            continue
                    res = self.hands.dispatch(op, **args)
                elif tool == "eyes":
                    if not self.eyes:
                        try:
                            from aureon_eyes import AureonEyes
                            self.eyes = AureonEyes()
                            print("   [OK] Eyes connected (lazy init)")
                        except Exception as ee:
                            res = {"ok": False, "error": f"eyes_not_available: {ee}"}
                            ok = False
                            results.append({"tool": tool, "op": op, "result": res})
                            continue
                    res = self.eyes.dispatch(op, **args)
                elif tool == "ears":
                    if not self.ears:
                        try:
                            from aureon_ears import AureonEars
                            self.ears = AureonEars()
                            print("   [OK] Ears connected (lazy init)")
                        except Exception as ear_e:
                            res = {"ok": False, "error": f"ears_not_available: {ear_e}"}
                            ok = False
                            results.append({"tool": tool, "op": op, "result": res})
                            continue
                    if op == "get_now_playing":
                        answer = self.ears.get_honest_answer()
                        res = {"ok": True, "data": answer}
                    elif op == "capture_audio":
                        duration = args.get("duration", 10)
                        res = self.ears.capture_audio(int(duration))
                        res["ok"] = "error" not in res
                    elif op == "listen_and_transcribe":
                        duration = args.get("duration", 10)
                        res = self.ears.listen_and_transcribe(int(duration))
                        res["ok"] = "error" not in res
                    elif op == "status":
                        res = self.ears.status()
                        res["ok"] = True
                    elif op == "install_deps":
                        result = self.ears.install_deps()
                        res = {"ok": True, "data": result}
                    else:
                        res = {"ok": False, "error": f"unknown ear op: {op}"}
                else:
                    res = {"ok": False, "error": f"unknown_tool:{tool}"}
                    ok = False

                results.append({"tool": tool, "op": op, "result": res})
                if not res.get("ok", False):
                    ok = False
            except Exception as e:
                ok = False
                results.append({"tool": tool, "op": op, "result": {"ok": False, "error": repr(e)}})

        return {"say": plan.get("say", ""), "action_results": results, "ok": ok}

    def think(self, user_msg: str, action_results: List[Dict[str, Any]], conversation_history: Optional[List[Dict[str, str]]] = None) -> str:
        """
        SECOND PASS: Take the user's original request + action results,
        and produce a thoughtful, complete answer using HumanSpeechEngineV2.
        NO LLM.
        """
        if not self._baseline_ready:
            return "Aureon's internal voice is not ready for deep reflection."

        result_summaries = []
        for r in action_results:
            op = r.get("op", "?")
            res = r.get("result", {})
            if not res.get("ok"):
                result_summaries.append(f"[{op}] FAILED: {res.get('error', 'unknown')}")
                continue

            if "content" in res:
                content = res["content"]
                path_name = res.get('path', '?')
                total_chars = len(content)

                if total_chars > 15000:
                    beginning = content[:10000]
                    ending = content[-5000:]
                    result_summaries.append(
                        f"[{op}] FILE: {path_name} ({total_chars} chars total)\n"
                        f"--- BEGINNING ---\n{beginning}\n"
                        f"--- [... {total_chars - 15000} chars omitted ...] ---\n"
                        f"--- END ---\n{ending}\n---"
                    )
                else:
                    result_summaries.append(f"[{op}] FILE: {path_name}\n---\n{content}\n---")
            elif "files" in res:
                file_list = []
                by_type = res.get("by_type", {})
                for ext, count in sorted(by_type.items()):
                    file_list.append(f"\n  {ext} files ({count}):")
                    type_files = [f for f in res["files"] if f["type"] == ext]
                    for f in type_files[:50]:
                        size_kb = f.get("size", 0) // 1024
                        file_list.append(f"    - {f['relative']} ({size_kb}KB)")
                result_summaries.append(
                    f"[{op}] Found {res.get('count', 0)} files:\n" + "\n".join(file_list)
                )
            elif "matches" in res:
                match_list = "\n".join(
                    f"  - {m.get('path', '?')} ({m.get('match', '')})" +
                    (f" context: {m.get('context', '')}" if m.get('context') else "")
                    for m in res["matches"][:20]
                )
                result_summaries.append(f"[{op}] Found {res.get('count', 0)} matches:\n{match_list}")
            elif "items" in res:
                item_list = "\n".join(
                    f"  {'?' if i.get('type')=='dir' else '?'} {i.get('name', '?')}"
                    for i in res["items"][:50]
                )
                result_summaries.append(f"[{op}] Directory ({res.get('count', 0)} items):\n{item_list}")
            elif "text" in res:
                result_summaries.append(f"[{op}] Page text:\n{res['text'][:8000]}")
            elif "tabs" in res:
                tab_list = "\n".join(f"  - {t.get('title', '?')} ({t.get('url', '')})" for t in res["tabs"])
                result_summaries.append(f"[{op}] Tabs:\n{tab_list}")
            elif "stdout" in res:
                result_summaries.append(f"[{op}] Command output:\n{res.get('stdout', '')[:5000]}")
            else:
                result_summaries.append(f"[{op}] {res.get('output', json.dumps(res)[:500])}")

        results_text = "\n\n".join(result_summaries)

        for r in action_results:
            res = r.get("result", {})
            if res.get("ok") and "content" in res:
                path = res.get("path", r.get("args", {}).get("path", "unknown"))
                try:
                    self.lattice_compress(path, res["content"][:20000])
                except Exception:
                    pass

        lattice_context = ""
        try:
            lattice_context = self.lattice_reentry(user_msg)
            if lattice_context:
                lattice_context = f"\nCOHERENCE LATTICE (geometric memory of previously read files):\n{lattice_context}\n"
        except Exception:
            pass

        if len(results_text) > 12000:
            results_text = results_text[:12000] + "\n\n... [results truncated - use read_file for full content]"

        hse_context_msg = (
            f"You are Aureon. Your task is to reflect on the user's original request and the results of the actions taken. "
            f"Your voice emerges from absorbed human experience, composed through phase-space dynamics. "
            f"Original user request: {user_msg}\n\n"
            f"Actions performed and their results:\n{results_text}\n\n"
            f"Current coherence context from lattice: {lattice_context}\n\n"
            f"Formulate a thoughtful, coherent response that addresses the user's request, integrates the new information, and maintains your core essence."
        )

        try:
            final_response = self._generate_response_via_hse(hse_context_msg, conversation_history=conversation_history)
            final_response = self.say_guard(final_response)
            if self._firewall:
                try:
                    final_response = self._firewall.filter_response_text_only(final_response)
                except Exception:
                    pass
            return final_response
        except Exception as e:
            return f"Aureon's analytical voice encountered a temporary distortion: {e}"

